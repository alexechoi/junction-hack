<img width="1104" height="639" alt="{1AEBB550-3E8D-40BD-B4A7-0D42D4ADA5D4}" src="https://github.com/user-attachments/assets/4e8936ad-7206-48ba-9ce2-c5202ece5e80" />

# Aegis - Trust Evaluation Platform
**"Security decisions in seconds"** - Aegis is an AI-powered security assessment platform that automates vendor trust evaluations. It generates instant, source-grounded reports with transparent trust scores, and includes an interactive chat agent to provide further insights and answer specific questions, enabling security teams to make fast, informed decisions.

## ğŸš€ Overview 
![Structural Diagram](https://github.com/user-attachments/assets/7266272e-81c7-4d69-935d-c50dfad74c58)
Aegis consists of two main components:
 - **Web-Client** (```junction-app/```) - A next.js web application providing an easy to use interface to evaluate products, discuss the evaluation with an agent, and a vault giving oversight over prior scoring.
 - **Deep Research Agent** (```deep_security/```) - An agent based on OpenAi & Langgraph, utilizing various specialist API's to thoroughly investigate the product provided.   

## Highlights
 - **ğŸ”’ Firebase Auth + Profiles** â€“ Email/password and Google SSO with enriched user metadata captured in Firestore.
 - **ğŸ“¥ Submission Hub** â€“ Text prompt + binary upload workflow for requesting assessments.
 - **ğŸ¤– Multi-LLM Research Agent** â€“ Configurable OpenAI/Anthropic stacks for summarizeâ†’researchâ†’compressâ†’report loops.
 - **ğŸ” Search + MCP Integrations** â€“ Pluggable Tavily, OpenAI native search, Anthropic native search, and custom MCP toolchains.
 - **ğŸ“Š Reports Vault** â€“ High-signal trust brief cards with risk tags, source counts, and sharing links.
 - **ğŸ§ª Benchmark Harness** â€“ Pre-wired Deep Research Bench evaluation scripts to validate agent quality.
## ğŸ—ï¸ Project Structure
```
junction-hack/
â”œâ”€â”€ junction-app/                # Next.js frontendâ”‚   
â”œâ”€â”€ app/                     # App Router routes (landing, auth, dashboard, reports)â”‚   
â”œâ”€â”€ components/              # Shared UI (AppChrome, landing sections)
â”‚   â”œâ”€â”€ contexts/AuthContext.tsx # Client-side auth/session provider
â”‚   â”œâ”€â”€ lib/firebase.ts          # Firebase initialization
â”‚   â”œâ”€â”€ public/                  # Static assets
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ deep_security/               # LangGraph / Open Deep Research backend
â”‚   â”œâ”€â”€ src/open_deep_research/  # Config + runtime
â”‚   â”œâ”€â”€ src/security/            # Auth helpers
â”‚   â”œâ”€â”€ tests/                   # Benchmark + evaluation scripts
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ pyproject.toml
â””â”€â”€ example_data.csv             # Sample assessment data
```
## ğŸ¨ Frontend (Next.js)
Modern App Router experience focusing on security analyst workflows:
 - Tech Stack: Next.js 15, TypeScript, Tailwind, shadcn/ui, Lucide icons.
 - Auth Flow: ```AuthContext``` wraps Firebase Auth; guards dashboard and reports routes.
 - Key Screens:
   - Landing page with hero/demo/trust-score highlights.
   - ```/auth``` multi-step login/register with Google SSO fallback.
   - ```/dashboard``` submission form (text + file upload) and quick links to reports.
   - ```/reports``` gallery of trust briefs with status, sources, and risk chips.
## âš™ï¸ Deep Research Service
LangGraph-backed agent toolbox housed in ```deep_security/```:
 - Configuration Surface: ```src/open_deep_research/configuration.py``` exposes sliders/toggles for structured-output retries, concurrency, model choices, search providers, and MCP settings.
 - Model Pipeline: Separate slots for summarization, researcher, compression, and final-report models (defaults to OpenAI ```gpt-4.1``` / ```gpt-4.1-mini```, but swappable to Anthropic, GPT-5, etc.).
 - Search & MCP: Built-in support for Tavily, OpenAI native, Anthropic native search plus external MCP servers for custom tools/data.
 - Evaluation: ```tests/run_evaluate.py``` and ```tests/extract_langsmith_data.py``` automate Deep Research Bench submissions (LangSmith integration).
## ğŸ§­ Data Flow
 1. User Authenticates â€“ Firebase Auth session hydrates ```AuthContext```.
 2. Submission â€“ Dashboard posts text/binary payload to a Next.js API route or edge function (placeholder today).
 3. Assessment Orchestration â€“ API proxies request to LangGraph runtime (Deep Research service).
 4. LLM + Search Loop â€“ Agent fans out to configured LLMs, search APIs, and MCP tools, storing intermediate notes.
 5. Report Storage â€“ Final trust brief, scores, and citation metadata saved back to Firestore.
 6. Consumption â€“ Reports UI reads Firestore entries for sharing/export.
## ğŸš€ Quick Start
### Prerequisites
 - Node.js 18+ (or Bun), npm/yarn/pnpm.
 - Python 3.11, ```uv``` or ```pip```.
 - Firebase project (Auth + Firestore) + service credentials.
 - OpenAI and/or Anthropic API keys (plus Tavily key if using default search).
 - LangSmith account if running benchmarks.
### Frontend Setup
```
cd junction-app
cp .env.example .env.local   # fill Firebase + API vars
npm installnpm
run dev
```
Visit [http://localhost:3000](http://localhost:3000).
### Backend Setup
```
cd deep_securityuv venv && source .venv/bin/activate    # or python -m venvuv 
sync                                 # installs LangChain/LangGraph dep
scp .env.example .env                    # configure LLM/search/MCP keys
uvx --from "langgraph-cli[inmem]" langgraph dev --allow-blocking
```
LangGraph Studio UI available at the printed URL (default [http://127.0.0.1:2024)](http://127.0.0.1:2024)).
## ğŸ› ï¸ Environment Variables
|Component|	Variable|	Description|
|--|--|--|
|junction-app|	NEXT_PUBLIC_FIREBASE_*|	Firebase web config (auth domain, project IDâ€¦)|
||NEXT_PUBLIC_ASSESSMENT_API_URL|	(Future) API route for submissions|
|deep_security|	SUMMARIZATION_MODEL, RESEARCH_MODELâ€¦	|Override default LLMs per stage|
||SEARCH_API|	tavily, openai, anthropic, or none|
||MCP_CONFIG_URL, MCP_CONFIG_TOOLS	|Optional MCP server info|
|Shared	|OPENAI_API_KEY, ANTHROPIC_API_KEY	|Provider credentials|
|Shared|	TAVILY_API_KEY	Web |search enrichment|
## ğŸ“š Documentation
- ```junction-app/README.md``` â€“ Frontend development tips.
- ```deep_security/README.md``` â€“ LangGraph configuration, benchmarking, LangSmith usage.
- LangChain docs for MCP + multi-provider LLM setup.
- Firebase docs for Auth/Firestore provisioning.
## ğŸš¢ Deployment
| Layer |	Recommended Target |
|--|--|
|Frontend	|Vercel / Netlify (set Firebase/public env vars)|
|API Routes |	Vercel Edge Functions or Next.js serverless runtime|
|LangGraph|	Dockerized service on cloud VM or LangGraph Platform|
|Firebase	|Managed (Auth + Firestore)|
 1. Build frontend: ```npm run build``` â†’ deploy.
 2. Package LangGraph service with ```uv``` + ```langgraph dev``` or containerize for production.
 3. Wire API route to call LangGraph service; secure with bearer tokens.
 4. Point frontend env vars to production endpoints.
## ğŸ§ª Testing & Evaluation
 - Frontend: ```npm run lint``` / ```npm run test``` (if configured) plus manual UI smoke tests.
 - Backend: Run ```python tests/run_evaluate.py``` for Deep Research Bench; extract results via tests/extract_langsmith_data.py.
 - Integration: Validate that Firestore entries appear when manual assessments are triggered (mock API route until backend is wired).
## ğŸ¤ Contributing
 1. Fork and branch (```git checkout -b feature/<name>```).
 2. Keep frontend TypeScript strict and follow existing Tailwind patterns.
 3. For backend changes, update ```configuration.py``` docs + README when adding config knobs.
 4. Add tests or LangSmith eval notes for new research behaviors.
 5. Submit PR with a concise summary and screenshots if UI-related.
## ğŸ“„ License
MIT â€“ see LICENSE.
## ğŸ™‹ Support & Questions
Open an issue in this repo.
Check LangGraph + Firebase docs linked above.
Reach out on project Slack/Discord (if applicable) for architecture questions.
